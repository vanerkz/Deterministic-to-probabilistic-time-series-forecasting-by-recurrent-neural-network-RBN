{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28881,
     "status": "ok",
     "timestamp": 1666122413113,
     "user": {
      "displayName": "Van Koh",
      "userId": "15908915889656003541"
     },
     "user_tz": -480
    },
    "id": "C3VXRzTvtuO-",
    "outputId": "8a582d77-aa7f-4b90-e9d9-cb267ecb08fc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(model='NRU_RBN', data='ETTh1', root_path='datasets', data_path='ETTh1.csv', features='MS', target='OT', freq='h', checkpoints='model_parameters/ETTh1', seq_len=91, label_len=90, pred_len=30, enc_in=7, dec_in=7, c_out=1, d_model=256, d_ff=256, padding=0, distil=False, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=True, mix=True, cols=None, num_workers=32, itr=3, train_epochs=20, batch_size=128, patience=2, learning_rate=0.0001, des='test', loss='mse', lradj='type1', use_amp=False, inverse=False, likeloss=True, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', lossregularizer=True, detail_freq='h')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from exp.exp_NRU_RBN import Exp_NRU_RBN\n",
    "%matplotlib inline\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NRU_RBN Forecasting')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='NRU_RBN',help='model of experiment, options: [NRU_RBN]')\n",
    "value=\"ETTh1\" #ETTh1,ETTh2,electrans,weather,exchange,BTC\n",
    "parser.add_argument('--data', type=str, default=value, help='data')\n",
    "parser.add_argument('--root_path', type=str, default='datasets', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default=value+'.csv', help='data file')    #electrans.csv\n",
    "parser.add_argument('--features', type=str, default='MS', help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default= 'model_parameters/'+value, help='location of model checkpoints')\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=(90)+1, help='input sequence length of Informer encoder')\n",
    "parser.add_argument('--label_len', type=int, default=90, help='start token length of Informer decoder')\n",
    "parser.add_argument('--pred_len', type=int, default=30, help='prediction sequence length')\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "parser.add_argument('--enc_in', type=int, default=3, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=3, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=3, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=256, help='dimension of model')\n",
    "parser.add_argument('--d_ff', type=int, default=256, help='dimension of fcn')\n",
    "parser.add_argument('--padding', type=int, default=0, help='padding type')\n",
    "parser.add_argument('--distil', action='store_false', help='whether to use distilling in encoder, using this argument means not using distilling', default=False)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu',help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder',default=False)\n",
    "parser.add_argument('--do_predict', action='store_false', help='whether to predict unseen future data')\n",
    "parser.add_argument('--mix', action='store_false', help='use mix attention in generative decoder', default=True)\n",
    "parser.add_argument('--cols', type=str, nargs='+', help='certain cols from the data files as the input features')\n",
    "parser.add_argument('--num_workers', type=int, default=32, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=3, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=20, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=2, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test',help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse',help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1',help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "parser.add_argument('--likeloss', action='store_true', help='likeloss', default=True)\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3',help='device ids of multile gpus')\n",
    "parser.add_argument('--lossregularizer', type=bool, default=True, help='Use loss regularizer')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "data_parser = {\n",
    "    'exchange':{'data':'exchange.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1],'freqin':'d'},\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1],'freqin':'h'},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1],'freqin':'h'},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1],'freqin':'h'},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1],'freqin':'h'},\n",
    "    'WTH':{'data':'WTH.csv','T':'WetBulbCelsius','M':[12,12,12],'S':[1,1,1],'MS':[12,12,1],'freqin':'h'},\n",
    "    'weather':{'data':'weather.csv','T':'WetBulbCelsius','M':[12,12,12],'S':[1,1,1],'MS':[12,12,1],'freqin':'h'},\n",
    "    'electrans':{'data':'electrans.csv','T':'MT_24','M':[316,316,316],'S':[1,1,1],'MS':[316,316,1],'freqin':'h'},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "    args.freq=data_info['freqin']\n",
    "\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "args.likeloss =args.lossregularizer\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 6370,
     "status": "error",
     "timestamp": 1666122435621,
     "user": {
      "displayName": "Van Koh",
      "userId": "15908915889656003541"
     },
     "user_tz": -480
    },
    "id": "Y1BfxLK90Gc4",
    "outputId": "512a24e4-be9e-4029-806d-812de71b624f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "Num_params : 3432394 \n",
      ">>>>>>>start training : id0_mNRU_RBN_dETTh1_ftMS_sl91_ll90_pl30_dm256_df256_ebtimeF_0_lossreg_True>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 12074\n",
      "val 2493\n",
      "test 2493\n",
      "No File, Train new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:50<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 50.65706419944763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_CRPS_mean:0.4144927 vali_CRPS_var:0.00033199313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:05<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_CRPS_mean:0.24613762 test_CRPS_var:0.010629237\n",
      "Epoch: 1, Steps: 94 | Train Loss: 0.8166580 Vali Loss: 0.7225042 Test Loss: 0.7980182\n",
      "Validation loss decreased (inf --> 0.722504).  Saving model ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model_parameters/ETTh1 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141530/4086126584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mruntrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mruntrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_141530/4086126584.py\u001b[0m in \u001b[0;36mruntrain\u001b[0;34m(index, args)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/TS_Former_test_MAP_NRU/exp/exp_NRU_RBN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    175\u001b[0m             print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n\u001b[1;32m    176\u001b[0m                 epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvali_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Early stopping\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/TS_Former_test_MAP_NRU/utils_NRU_RBN/tools.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, val_loss, model, path)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/TS_Former_test_MAP_NRU/utils_NRU_RBN/tools.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, val_loss, model, path)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory model_parameters/ETTh1 does not exist."
     ]
    }
   ],
   "source": [
    "#test&train\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mape_list=[]\n",
    "\n",
    "mae_list=[]\n",
    "rmse_list=[]\n",
    "dtw_list=[]\n",
    "res_list=[]\n",
    "shape_list=[]\n",
    "corr_list=[]\n",
    "crps_list=[]\n",
    "mse_list=[]\n",
    "crps_varlist=[]\n",
    "\n",
    "Exp = Exp_NRU_RBN\n",
    "def runtrain(index,args):\n",
    "    for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = 'id{}_m{}_d{}_ft{}_sl{}_ll{}_pl{}_dm{}_df{}_eb{}_{}_lossreg_{}'.format(\n",
    "                    index,args.model, args.data, args.features,args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.d_ff, \n",
    "                    args.embed, 0,str(args.lossregularizer))\n",
    "\n",
    "            exp = Exp(args) # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        \n",
    "            exp.train(setting)\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            \n",
    "            resultout,_ ,_,crpsret=exp.test(setting,False)\n",
    "            mae, mse, rmse, mape,_, dtw,corr,shapeval =resultout\n",
    "            attns=0\n",
    "            mse_list.append(mse)\n",
    "            mse_np=np.array(mse_list)\n",
    "            mae_list.append(mae)\n",
    "            mae_np=np.array(mae_list)\n",
    "            crps_list.append(np.mean(crpsret))\n",
    "            crps_varlist.append(np.var(crpsret))\n",
    "            crps_np=np.array(crps_list)\n",
    "            crps_varlist_np=np.array(crps_varlist)\n",
    "            print(\"Index:\"+str(index)+\",CRPS_L:\"+str(len(crps_np))+\",AVE:\"+str(np.mean(crps_np))+\",var:\"+str(np.var(crps_varlist_np)))\n",
    "            print(\"Index:\"+str(index)+\",MSE_L:\"+str(len(mse_np))+\",AVE:\"+str(np.mean(mse_np))+\",var:\"+str(np.var(mse_np)))\n",
    "            print(\"Index:\"+str(index)+\",MAE_L:\"+str(len(mae_np))+\",AVE:\"+str(np.mean(mae_np))+\",var:\"+str(np.var(mae_np)))\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if args.data_path == 'electrans.csv' or args.data_path == 'exchange.csv':\n",
    "        df_raw = pd.read_csv(os.path.join(args.root_path,\n",
    "                                          args.data+\"_all.csv\")).set_index('date')\n",
    "        t,c=df_raw.shape\n",
    "        for index in range(c):\n",
    "                print(\"Index: \", index)\n",
    "                df_use = df_raw.iloc[:,index]\n",
    "                path = \"datasets/\"+args.data+\".csv\"\n",
    "                df_use.to_csv(path)\n",
    "                runtrain(index,args)\n",
    "else:\n",
    "        runtrain(0,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Exp_NRU_RBN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_134889/1773235068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcrps_varlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mExp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExp_NRU_RBN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mruntrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     setting = 'id{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}_lossreg_{}'.format(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Exp_NRU_RBN' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#test&train\n",
    "\n",
    "\n",
    "mape_list=[]\n",
    "\n",
    "mae_list=[]\n",
    "rmse_list=[]\n",
    "dtw_list=[]\n",
    "res_list=[]\n",
    "shape_list=[]\n",
    "corr_list=[]\n",
    "crps_list=[]\n",
    "mse_list=[]\n",
    "crps_varlist=[]\n",
    "\n",
    "Exp = Exp_NRU_RBN\n",
    "def runtrain(index,args):\n",
    "    \n",
    "    setting = 'id{}_m{}_d{}_ft{}_sl{}_ll{}_pl{}_dm{}_df{}_eb{}_{}_lossreg_{}'.format(\n",
    "                    index,args.model, args.data, args.features,args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.d_ff, \n",
    "                    args.embed, 0,str(args.lossregularizer))exp = Exp(args)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    resultout,_ ,_,crpsret=exp.test(setting,True)\n",
    "    mae, mse, rmse, mape,_, dtw,corr,shapeval =resultout\n",
    "    attns=0\n",
    "    \"\"\"if args.do_predict:\n",
    "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.predict(setting, True)\"\"\"\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return crpsret,mae, mse, rmse, mape,_, dtw,corr,shapeval\n",
    "\n",
    "\n",
    "if args.data_path == 'electrans.csv'or args.data_path == 'exchange.csv':\n",
    "        df_raw = pd.read_csv(os.path.join(args.root_path,\n",
    "                                          args.data+\"_all.csv\")).set_index('date')\n",
    "        t,c=df_raw.shape\n",
    "        for index in range(0, c):\n",
    "                print(\"Index: \", index)\n",
    "                df_use = df_raw.iloc[:,index]\n",
    "                path = \"datasets/\"+args.data+\".csv\"\n",
    "                df_use.to_csv(path)\n",
    "                crpsret,mae, mse, rmse, mape,_, dtw,corr,shapeval=runtrain(index,args)\n",
    "                mse_list.append(mse)\n",
    "                mse_np=np.array(mse_list)\n",
    "                mae_list.append(mae)\n",
    "                mae_np=np.array(mae_list)\n",
    "                crps_list.append(np.mean(crpsret))\n",
    "                crps_varlist.append(np.var(crpsret))\n",
    "                crps_np=np.array(crps_list)\n",
    "                crps_varlist_np=np.array(crps_varlist)\n",
    "                print(\"Index:\"+str(index)+\",CRPS_L:\"+str(len(crps_np))+\",AVE:\"+str(np.mean(crps_np))+\",var:\"+str(np.var(crps_varlist_np)))\n",
    "                print(\"Index:\"+str(index)+\",MSE_L:\"+str(len(mse_np))+\",AVE:\"+str(np.mean(mse_np))+\",var:\"+str(np.var(mse_np)))\n",
    "                print(\"Index:\"+str(index)+\",MAE_L:\"+str(len(mae_np))+\",AVE:\"+str(np.mean(mae_np))+\",var:\"+str(np.var(mae_np)))\n",
    "\n",
    "else:\n",
    "        crpsret,mae, mse, rmse, mape,_, dtw,corr,shapeval=runtrain(0,args)\n",
    "        mse_list.append(mse)\n",
    "        mse_np=np.array(mse_list)\n",
    "        mae_list.append(mae)\n",
    "        mae_np=np.array(mae_list)\n",
    "        crps_list.append(np.mean(crpsret))\n",
    "        crps_varlist.append(np.var(crpsret))\n",
    "        crps_np=np.array(crps_list)\n",
    "        crps_varlist_np=np.array(crps_varlist)\n",
    "        print(\"Index:\"+str(0)+\",CRPS_L:\"+str(len(crps_np))+\",AVE:\"+str(np.mean(crps_np))+\",var:\"+str(np.var(crps_varlist_np)))\n",
    "        print(\"Index:\"+str(0)+\",MSE_L:\"+str(len(mse_np))+\",AVE:\"+str(np.mean(mse_np))+\",var:\"+str(np.var(mse_np)))\n",
    "        print(\"Index:\"+str(0)+\",MAE_L:\"+str(len(mae_np))+\",AVE:\"+str(np.mean(mae_np))+\",var:\"+str(np.var(mae_np)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:0,CRPS_L:321,AVE:0.17263213,var:0.00015839086\n",
      "Index:0,MSE_L:321,AVE:0.13705546,var:0.020117808\n",
      "Index:0,MAE_L:321,AVE:0.16893287,var:0.011108408\n"
     ]
    }
   ],
   "source": [
    "print(\"Index:\"+str(0)+\",CRPS_L:\"+str(len(crps_np))+\",AVE:\"+str(np.mean(crps_np))+\",var:\"+str(np.var(crps_varlist_np)))\n",
    "print(\"Index:\"+str(0)+\",MSE_L:\"+str(len(mse_np))+\",AVE:\"+str(np.mean(mse_np))+\",var:\"+str(np.var(mse_np)))\n",
    "print(\"Index:\"+str(0)+\",MAE_L:\"+str(len(mae_np))+\",AVE:\"+str(np.mean(mae_np))+\",var:\"+str(np.var(mae_np)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (801630156.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1362454/801630156.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Index:0,CRPS_L:1,AVE:0.18990368,var:0.0\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#MS\n",
    "#ETTh1:\n",
    "Index:0,CRPS_L:1,AVE:0.18990368,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.12117207,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.2073799,var:0.0024236497\n",
    "#ETTh2\n",
    "Index:0,CRPS_L:1,AVE:0.2722967,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.24472795,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.28396863,var:0.007662289\n",
    "#Weather\n",
    "Index:0,CRPS_L:1,AVE:0.18372725,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.1277009,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.19873646,var:0.0029750222\n",
    "\n",
    "#WO REG\n",
    "#Exchange\n",
    "Index:7,CRPS_L:8,AVE:0.31373262,var:0.0034177054\n",
    "Index:7,MSE_L:8,AVE:0.37887412,var:0.190408\n",
    "Index:7,MAE_L:8,AVE:0.32387847,var:0.0671021\n",
    "\n",
    "#ETTh1\n",
    "Index:0,CRPS_L:1,AVE:0.16855828,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.09799735,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.18251573,var:0.002681852\n",
    "#ETTh2\n",
    "Index:0,CRPS_L:1,AVE:0.26861325,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.2357498,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.27262577,var:0.010254114\n",
    "#weather\n",
    "Index:0,CRPS_L:1,AVE:0.19781663,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.15007746,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.22374451,var:0.0024221768\n",
    "\n",
    "#_________________________________________________\n",
    "#W REG\n",
    "#Single\n",
    "#Exchange\n",
    "Index:7,CRPS_L:8,AVE:0.23455146,var:0.0013202557\n",
    "Index:7,MSE_L:8,AVE:0.20312688,var:0.038856484\n",
    "Index:7,MAE_L:8,AVE:0.25125167,var:0.024895169\n",
    "#Elecrans\n",
    "Index:0,CRPS_L:321,AVE:0.1714941,var:0.00011770314\n",
    "Index:0,MSE_L:321,AVE:0.13653912,var:0.021596305\n",
    "Index:0,MAE_L:321,AVE:0.16777086,var:0.010939249\n",
    "\n",
    "#ETTh1\n",
    "Index:0,CRPS_L:1,AVE:0.1713251,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.101203896,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.1860563,var:0.0026892824\n",
    "\n",
    "#ETTh2\n",
    "Index:0,CRPS_L:1,AVE:0.26525223,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.23398851,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.27045283,var:0.01107575\n",
    "\n",
    "#Weather\n",
    "Index:0,CRPS_L:1,AVE:0.19394289,var:0.0\n",
    "Index:0,MSE_L:1,AVE:0.1439406,var:0.0\n",
    "Index:0,MAE_L:1,AVE:0.21706055,var:0.0025334116"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYamTAUmYDsZgII9uXX8CT",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1bcfz1OsVbD2yCIk1zcleV5paW0iaFtTg",
   "name": "",
   "version": ""
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "60461829a35a4f24414f6b9a81cb167855ac5d995cbf4b4b96212d9b44c21eb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
